{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de9a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tahid Hasan\\Desktop\\Lang document\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e2f97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4890ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9912e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    SystemMessage(content='You are a helpful assistant'),\n",
    "    HumanMessage(content='Tell me about Langchain')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b147c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d22e0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyC-T9wKePVqJnkyfaC24SbwLamtsSCsltQ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb222cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "message.append(AIMessage(content = result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724574c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Tell me about Langchain', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It provides a structured way to connect LLMs with other data sources and tools, allowing developers to build more complex and powerful applications than would be possible with just a standalone LLM.\\n\\nThink of it as the **orchestrator** for your LLM applications. While LLMs are incredibly powerful, they have limitations (e.g., lack of up-to-date information, inability to perform actions, limited context window). LangChain helps overcome these limitations by providing tools to:\\n\\n1.  **Connect LLMs to external data:** Give LLMs access to your own documents, databases, or the internet.\\n2.  **Allow LLMs to interact with their environment:** Enable LLMs to use tools like calculators, search engines, or custom APIs.\\n3.  **Manage and chain multiple LLM calls:** Break down complex tasks into smaller, manageable steps.\\n\\n### Why was LangChain created? (The Problem it Solves)\\n\\nLLMs, in their raw form, are excellent at generating text based on their training data. However, they typically:\\n*   **Lack up-to-date information:** Their knowledge is capped at their last training cut-off date.\\n*   **Lack domain-specific knowledge:** They don\\'t know about your company\\'s internal documents or specific industry jargon.\\n*   **Cannot perform actions:** They can\\'t browse the web, run code, or interact with external systems.\\n*   **Have limited context windows:** They can only \"remember\" a certain amount of previous conversation or input.\\n\\nLangChain provides the building blocks to address these issues, turning a \"text generator\" into a more capable \"reasoning engine\" that can interact with the real world.\\n\\n### Key Components of LangChain:\\n\\nLangChain is built around several core modules, each serving a specific purpose:\\n\\n1.  **Models:**\\n    *   **LLMs:** Integrations with various language models (e.g., OpenAI, Anthropic, Hugging Face models). This is the core \"brain\" of your application.\\n    *   **Chat Models:** Optimized for conversational interactions, handling message histories.\\n    *   **Embedding Models:** Convert text into numerical vectors (embeddings), crucial for similarity search and retrieval.\\n\\n2.  **Prompts:**\\n    *   **Prompt Templates:** Reusable and customizable templates for generating prompts to LLMs. This helps in consistent and effective communication with the model.\\n    *   **Output Parsers:** Structure the LLM\\'s raw text output into more usable formats (e.g., JSON, Pydantic objects).\\n\\n3.  **Chains:**\\n    *   These are sequences of modular components that can be combined to perform more complex tasks. A chain might involve multiple LLM calls, data processing steps, or tool usages.\\n    *   **Example:** An `LLMChain` combines a prompt template with an LLM. A `SimpleSequentialChain` runs multiple chains in order.\\n\\n4.  **Retrieval:**\\n    *   **Document Loaders:** Load data from various sources (PDFs, websites, databases, CSVs, etc.).\\n    *   **Text Splitters:** Break large documents into smaller, manageable chunks suitable for LLM context windows and embedding.\\n    *   **Vector Stores:** Databases designed to store and search vector embeddings (e.g., Pinecone, Chroma, FAISS).\\n    *   **Retrievers:** Fetch relevant documents or data chunks from external sources (often vector stores) based on a query, which can then be fed to the LLM. This is fundamental for **Retrieval Augmented Generation (RAG)**.\\n\\n5.  **Agents:**\\n    *   Perhaps one of the most powerful features. Agents allow an LLM to decide *what action to take* and *when*, based on user input and available tools.\\n    *   The LLM acts as a \"reasoning engine,\" observing the environment, thinking about what to do, performing an action using a **tool** (e.g., Google Search, a calculator, a custom API call), observing the result, and then iterating. This enables dynamic and adaptive behavior.\\n\\n6.  **Memory:**\\n    *   Allows an LLM application to remember past interactions and maintain context over a conversation. This is crucial for building stateful chatbots.\\n    *   Various types of memory exist, from simple buffer memory (recalling recent messages) to more complex summary memory (summarizing past conversations).\\n\\n### Common Use Cases for LangChain:\\n\\n*   **Question Answering over Documents (RAG):** Asking questions about your own PDFs, internal knowledge bases, or web pages.\\n*   **Chatbots:** Building conversational AI that can maintain context and interact with external systems.\\n*   **Data Analysis:** Using LLMs to interpret and analyze structured or unstructured data, potentially running code or queries.\\n*   **Code Generation and Debugging:** Creating tools that can write, explain, or debug code.\\n*   **Personal Assistants:** Automating tasks by connecting LLMs to various APIs (e.g., calendar, email, task managers).\\n*   **Summarization and Extraction:** Summarizing long documents or extracting specific information.\\n\\n### Benefits of Using LangChain:\\n\\n*   **Abstraction and Modularity:** Simplifies complex interactions with LLMs and external systems.\\n*   **Flexibility:** Supports a wide range of LLMs, data sources, and tools.\\n*   **Rapid Prototyping:** Allows developers to quickly build and iterate on LLM-powered applications.\\n*   **Standardization:** Provides a common interface for different components, making it easier to swap them out.\\n*   **Community and Ecosystem:** An active open-source community provides resources, examples, and ongoing development.\\n\\n### Limitations/Considerations:\\n\\n*   **Complexity:** Can have a steep learning curve due to the number of components and concepts.\\n*   **Overhead:** For very simple LLM calls, it might introduce unnecessary complexity.\\n*   **Debugging:** Multi-step chains and agentic behavior can be harder to debug than simpler scripts.\\n*   **Performance:** Chaining many steps can introduce latency.\\n*   **Rapid Evolution:** The framework is under active development, meaning APIs and best practices can change frequently.\\n\\nIn essence, LangChain empowers developers to move beyond simple LLM prompts and build sophisticated, intelligent applications that can reason, retrieve information, and interact with the world, making LLMs truly useful in a wider array of real-world scenarios.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
