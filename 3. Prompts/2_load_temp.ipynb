{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b17ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f565d7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994a9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = load_prompt('common.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fd1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f88f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = pro.invoke(\n",
    "    {\n",
    "        'paper_title' : 'Attention Is All You Need',\n",
    "        'style_input' : 'describe marhamatically',\n",
    "        'length_input' : 50\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3908eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Transformer model relies solely on Scaled Dot-Product Attention: $Attention(Q, K, V) = softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})V$. This mechanism computes weighted sums of values, where weights are determined by query-key similarity. Analogous to highlighting crucial parts of a sentence, it enables parallel processing, replacing sequential recurrence.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--a7cacf11-3031-4b60-8a6f-976c727c108a-0', usage_metadata={'input_tokens': 142, 'output_tokens': 1403, 'total_tokens': 1545, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1327}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e3e13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = pro | model\n",
    "result = chain.invoke({\n",
    "        'paper_title' : 'Attention Is All You Need',\n",
    "        'style_input' : 'describe marhamatically',\n",
    "        'length_input' : 50\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40ad323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer model relies solely on `Attention(Q, K, V) = softmax((QK^T / sqrt(d_k))V)`. This multi-head self-attention mechanism calculates how much each input token influences others. Positional encodings, like `PE(pos, i) = sin(pos/10000^(2i/d_model))`, infuse sequence order, eliminating recurrent layers entirely.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
